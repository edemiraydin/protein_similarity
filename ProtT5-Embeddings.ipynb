{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4348c3d-8730-40f0-badc-cf7b4c66a367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘protT5’: File exists\n",
      "mkdir: cannot create directory ‘protT5/protT5_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/sec_struct_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/output’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir protT5 # root directory for storing checkpoints, results and fasta files with sample proteins\n",
    "!mkdir protT5/protT5_checkpoint\n",
    "!mkdir protT5/sec_struct_checkpoint \n",
    "!mkdir protT5/output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8c4078-596e-46fb-9505-90dd86dc7113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_path = \"./protT5/proteins.fasta\"\n",
    "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "609a51f0-58af-4d19-84bb-eb4d7a54031e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  6 20:34:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                     Off| 00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   19C    P8               16W / 300W|      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A10G                     Off| 00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   21C    P0               41W / 300W|      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A10G                     Off| 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   21C    P0               41W / 300W|      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A10G                     Off| 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   21C    P0               41W / 300W|      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4d6fe2-d535-467b-b2f1-2756422e1bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d0d446-5c8d-4df7-baa8-6ef19172bbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet( torch.nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # This is only called \"elmo_feature_extractor\" for historic reason\n",
    "        # CNN weights are trained on ProtT5 embeddings\n",
    "        self.elmo_feature_extractor = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( 1024, 32, kernel_size=(7,1), padding=(3,0) ), # 7x32\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Dropout( 0.25 ),\n",
    "                        )\n",
    "        n_final_in = 32\n",
    "        self.dssp3_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 3, kernel_size=(7,1), padding=(3,0)) # 7\n",
    "                        )\n",
    "        \n",
    "        self.dssp8_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 8, kernel_size=(7,1), padding=(3,0))\n",
    "                        )\n",
    "        self.diso_classifier = torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d( n_final_in, 2, kernel_size=(7,1), padding=(3,0))\n",
    "                        )\n",
    "        \n",
    "\n",
    "    def forward( self, x):\n",
    "        # IN: X = (B x L x F); OUT: (B x F x L, 1)\n",
    "        x = x.permute(0,2,1).unsqueeze(dim=-1) \n",
    "        x         = self.elmo_feature_extractor(x) # OUT: (B x 32 x L x 1)\n",
    "        d3_Yhat   = self.dssp3_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 3)\n",
    "        d8_Yhat   = self.dssp8_classifier( x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 8)\n",
    "        diso_Yhat = self.diso_classifier(  x ).squeeze(dim=-1).permute(0,2,1) # OUT: (B x L x 2)\n",
    "        return d3_Yhat, d8_Yhat, diso_Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec149703-04bf-46e9-965c-dcd0563a02ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd6a98d-afcd-4acd-ba4e-3d5396143b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_fasta( fasta_path, split_char=\"!\", id_field=0):\n",
    "    seqs = dict()\n",
    "    with open( fasta_path, 'r' ) as fasta_f:\n",
    "        for line in fasta_f:\n",
    "            # get uniprot ID from header and create new entry\n",
    "            if line.startswith('>'):\n",
    "                uniprot_id = line.replace('>', '').strip().split(split_char)[id_field]\n",
    "                # replace tokens that are mis-interpreted when loading h5\n",
    "                uniprot_id = uniprot_id.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "                seqs[ uniprot_id ] = ''\n",
    "            else:\n",
    "                # repl. all whie-space chars and join seqs spanning multiple lines, drop gaps and cast to upper-case\n",
    "                seq= ''.join( line.split() ).upper().replace(\"-\",\"\")\n",
    "                # repl. all non-standard AAs and map them to unknown/X\n",
    "                seq = seq.replace('U','X').replace('Z','X').replace('O','X')\n",
    "                seqs[ uniprot_id ] += seq \n",
    "    example_id=next(iter(seqs))\n",
    "    print(\"Read {} sequences.\".format(len(seqs)))\n",
    "    print(\"Example:\\n{}\\n{}\".format(example_id,seqs[example_id]))\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e08b44-da4c-4d04-ad94-22d52b8168cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings( model, tokenizer, seqs, per_protein,\n",
    "                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
    "\n",
    "    results = {\n",
    "               \"protein_embs\" : dict()\n",
    "              }\n",
    "\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "\n",
    "        # count residues in current batch and add the last sequence length to\n",
    "        # avoid that batches with (n_res_batch > max_residues) get processed \n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len \n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "\n",
    "            # add_special_tokens adds extra token at the end of each sequence\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "               \n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "                continue\n",
    "\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                \n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
    "                if per_protein: \n",
    "                    protein_emb = emb.mean(dim=0)\n",
    "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0a2e4c-20cf-489f-8723-3d8f34d91650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_embeddings(emb_dict,out_path):\n",
    "    with h5py.File(str(out_path), \"w\") as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a985ef-b23f-4c83-b518-8c1fd4494d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2 sequences.\n",
      "Example:\n",
      "sp|P04637|P53_HUMAN Cellular tumor antigen p53 OS=Homo sapiens OX=9606 GN=TP53 PE=1 SV=4\n",
      "MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPGSTKRALPNNTSSSPQPKKKPLDGEYFTLQIRGRERFEMFRELNEALELKDAQAGKEPGGSRAHSSHLKSKKGQSTSRHKKLMFKTEGPDSD\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder part of ProtT5-XL-U50 in half-precision (recommended)\n",
    "model, tokenizer = get_T5_model()\n",
    "seqs = read_fasta( seq_path )\n",
    "\n",
    "# Compute embeddings and/or secondary structure predictions\n",
    "results = get_embeddings( model, tokenizer, seqs, True)\n",
    "save_embeddings(results[\"protein_embs\"], per_protein_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b40a86-ed74-4310-9886-e5f84052e961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: snowflake-snowpark-python[pandas] in /usr/local/lib/python3.8/dist-packages (1.13.0)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (4.5.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (0.38.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (6.0)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (65.5.1)\n",
      "Requirement already satisfied: cloudpickle!=2.1.0,!=2.2.0,<=2.2.1,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-snowpark-python[pandas]) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (3.1.0)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.15.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2.28.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (23.0)\n",
      "Requirement already satisfied: tomlkit in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (0.12.4)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (3.4)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.5.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (3.1.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2022.7.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.26.14)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (3.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2022.12.7)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (24.0.0)\n",
      "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (42.0.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2.21)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (10.0.1.dev0+ga6eabc2b.d20230220)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas<3.0.0,>=1.0.0->snowflake-connector-python<4.0.0,>=3.6.0->snowflake-snowpark-python[pandas]) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-snowpark-python[pandas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e0c8d7-db3a-4a8f-9756-ab4eac52f345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/snowflake/connector/options.py:103: UserWarning: You have an incompatible version of 'pyarrow' installed (10.0.1.dev0+ga6eabc2b.d20230220), please install a version that adheres to: 'pyarrow; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time \n",
    "import requests\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.exceptions import *\n",
    "import snowflake.connector\n",
    "from snowflake.snowpark.types import VectorType, StructType, StructField\n",
    "from snowflake.snowpark.functions import col, lit, vector_l2_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf996ed2-b1e0-47c4-8fbd-d4da88ac8068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token():\n",
    "    with open('/snowflake/session/token', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "connection_params = {\n",
    "    'host': os.environ['SNOWFLAKE_HOST'],\n",
    "    'port': os.environ['SNOWFLAKE_PORT'],\n",
    "    'protocol': 'https',\n",
    "    'account': os.environ['SNOWFLAKE_ACCOUNT'],\n",
    "    'authenticator': 'oauth',\n",
    "    'token': get_token(),\n",
    "    'role': 'SYSADMIN',\n",
    "    'warehouse': 'COMPUTE_WH',\n",
    "    'database': 'BIONEMO_DB',\n",
    "    'schema': 'PUBLIC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb3721d-094a-4ea6-b264-ee57b451815c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_params).create()\n",
    "token = get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e59e15e-54f1-4dce-b78f-d22f3cd64015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import numpy as np\n",
    "import h5py\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "datasets = []\n",
    "keys =[]\n",
    "#i=0\n",
    "with h5py.File('./protT5/output/per_protein_embeddings.h5', 'r') as f: \n",
    "    for file_key in f.keys():\n",
    "        #i=i+1\n",
    "        group = f[file_key]\n",
    "        #if i==101:\n",
    "          #break;\n",
    "        if isinstance(group, h5py._hl.dataset.Dataset):\n",
    "            keys.append(np.array(file_key))\n",
    "            \n",
    "            datasets.append(np.array(group))\n",
    "           # print(np.array(group))\n",
    "            continue\n",
    "        for group_key in group.keys():\n",
    "            #print(group_key)\n",
    "            group2 = group[group_key]\n",
    "            #print(group2)\n",
    "            if isinstance(group2, h5py._hl.dataset.Dataset):\n",
    "                keys.append(np.array(file_key))\n",
    "                datasets.append(np.array(group2))\n",
    "                continue\n",
    "            for group_key2 in group2.keys():\n",
    "                group3 = group2[group_key2]\n",
    "                #print(group3)\n",
    "                if isinstance(group3, h5py._hl.dataset.Dataset):\n",
    "                   keys.append(np.array(file_key))\n",
    "                   datasets.append(np.array(group3))\n",
    "                   continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a22fe3-ee12-47d7-8b4a-e16a7a5b9b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bionemo in /usr/local/lib/python3.8/dist-packages (0.3.1.post1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bionemo) (1.22.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from bionemo) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from bionemo) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from bionemo) (4.5.0)\n",
      "Requirement already satisfied: requests-futures in /usr/local/lib/python3.8/dist-packages (from bionemo) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->bionemo) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->bionemo) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->bionemo) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->bionemo) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bionemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b11bac5d-f895-4455-9158-9d1f7e399195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_KEY=\"<your_key>\"\n",
    "API_HOST=\"https://api.bionemo.ngc.nvidia.com/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ec5c0b-4c3e-454c-b355-73a219066888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'openfold', 'category': 'protein-structure', 'methods': ['predict']},\n",
       " {'name': 'alphafold2',\n",
       "  'category': 'protein-structure',\n",
       "  'methods': ['predict']},\n",
       " {'name': 'esmfold',\n",
       "  'category': 'protein-structure',\n",
       "  'methods': ['predict-no-aln']},\n",
       " {'name': 'jackhmmer', 'category': 'msa-calculation', 'methods': ['align']},\n",
       " {'name': 'protgpt2', 'category': 'protein-sequence', 'methods': ['generate']},\n",
       " {'name': 'moflow',\n",
       "  'category': 'molecule',\n",
       "  'methods': ['embeddings', 'decoder', 'generate']},\n",
       " {'name': 'megamolbart',\n",
       "  'category': 'molecule',\n",
       "  'methods': ['embeddings', 'generate']},\n",
       " {'name': 'molmim',\n",
       "  'category': 'molecule',\n",
       "  'methods': ['embeddings', 'decoder', 'generate']},\n",
       " {'name': 'diffdock',\n",
       "  'category': 'molecular-docking',\n",
       "  'methods': ['generate']},\n",
       " {'name': 'esm2-650m',\n",
       "  'category': 'protein-embedding',\n",
       "  'methods': ['embeddings']},\n",
       " {'name': 'esm2-3b',\n",
       "  'category': 'protein-embedding',\n",
       "  'methods': ['embeddings']},\n",
       " {'name': 'esm2-15b',\n",
       "  'category': 'protein-embedding',\n",
       "  'methods': ['embeddings']},\n",
       " {'name': 'esm1nv',\n",
       "  'category': 'protein-embedding',\n",
       "  'methods': ['embeddings']}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bionemo.api import BionemoClient\n",
    "api = BionemoClient(api_key=API_KEY, api_host=API_HOST)\n",
    "models=api.list_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5043be3-fc55-457f-a0fc-189533c532b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"UNIPROTID\"  |\"EMB::VECTOR(FLOAT,1024)\"                           |\"[0.01560472, 0.038481202, 0.020373046, 0.01996...  |\"DISTANCE\"             |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|P04637       |[0.01561999972909689, 0.03844999894499779, 0.02...  |[0.015604720450937748, 0.038481201976537704, 0....  |0.0006604380983691511  |\n",
      "|Q9TTA1       |[0.014779999852180481, 0.04149999842047691, 0.0...  |[0.015604720450937748, 0.038481201976537704, 0....  |0.085698190101902      |\n",
      "|P56424       |[0.012190000154078007, 0.03472999855875969, 0.0...  |[0.015604720450937748, 0.038481201976537704, 0....  |0.0992041789266408     |\n",
      "|P61260       |[0.012190000154078007, 0.03472999855875969, 0.0...  |[0.015604720450937748, 0.038481201976537704, 0....  |0.0992041789266408     |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3): #len(datasets)-1\n",
    "    v=list(datasets[i])\n",
    "    df=session.sql(f'''SELECT uniprotid, emb::VECTOR(FLOAT,1024), {v}::VECTOR(FLOAT,1024),\n",
    "                        VECTOR_L2_DISTANCE(emb, {v}::VECTOR(FLOAT,1024)) as distance\n",
    "                   from BIONEMO_DB.PUBLIC.PROTEINS\n",
    "                   order by distance asc\n",
    "                   limit 4''')\n",
    "    df.show()\n",
    "    proteins_df=df.select(col('UNIPROTID')).to_pandas()\n",
    "    p=proteins_df.values.tolist()\n",
    "    for i in range(1, len(p)):\n",
    "        original_sequence = api.get_uniprot(p[i][0])\n",
    "        original_result=api.openfold_sync(original_sequence)\n",
    "        pdb_filename=f'''pdb/{p[i][0]}.pdb'''\n",
    "        with open(pdb_filename, \"w\") as f:\n",
    "            f.write(original_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865cf1d-402e-4f21-acf9-1fd7963b2b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
